# Sugestões de Referências Científicas para Posts do Blog

## Posts já com referências ✅

### 2025-11-25-ensemble-optimization.md
- [1] Henzi et al. (2021) - Unbiased sampling (já adicionado)
- [2] Liu et al. (2024) - KAN Networks (já adicionado)
- [3] Liu et al. (2024) - KAN 2.0 (já adicionado)

### 2025-11-26-intelligent-auto-recovery.md
- [1][2] Liu et al. (2024) - KAN Networks (já adicionado)

## Sugestões para outros posts

### 2025-11-22-ml-powered-intelligence.md
**Tópicos:** Intent classification, lightweight ML, cost optimization

**Referências sugeridas:**
1. **Logistic Regression for text classification:**
   - Fan, R. E., Chang, K. W., Hsieh, C. J., Wang, X. R., & Lin, C. J. (2008). LIBLINEAR: A library for large linear classification. *Journal of Machine Learning Research*, 9(Aug), 1871-1874.
   - https://www.jmlr.org/papers/v9/fan08a.html

2. **Hybrid ML systems (lightweight + heavy models):**
   - Teerapittayanon, S., McDanel, B., & Kung, H. T. (2016). BranchyNet: Fast inference via early exiting from deep neural networks. *ICPR 2016*.
   - https://arxiv.org/abs/1709.01686

3. **Model compression and efficiency:**
   - Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*.
   - https://arxiv.org/abs/1503.02531

### 2025-11-20-advanced-context-management.md
**Tópicos:** Dependency graphs, PageRank, context optimization

**Referências sugeridas:**
1. **PageRank algorithm:**
   - Page, L., Brin, S., Motwani, R., & Winograd, T. (1999). The PageRank citation ranking: Bringing order to the web. *Stanford InfoLab*.
   - http://ilpubs.stanford.edu:8090/422/

2. **Code dependency analysis:**
   - Guo, P. J., Zimmermann, T., Nagappan, N., & Murphy, B. (2010). Characterizing and predicting which bugs get fixed: an empirical study of Microsoft Windows. *ICSE 2010*.
   - https://doi.org/10.1145/1806799.1806810

3. **Program analysis for context:**
   - Li, Y., Tarlow, D., Brockschmidt, M., & Zemel, R. (2015). Gated graph sequence neural networks. *ICLR 2016*.
   - https://arxiv.org/abs/1511.05493

### 2025-11-19-model-performance-benchmarks.md
**Tópicos:** LLM benchmarks for coding, HumanEval, SWE-Bench

**Referências sugeridas:**
1. **HumanEval benchmark:**
   - Chen, M., Tworek, J., Jun, H., Yuan, Q., et al. (2021). Evaluating large language models trained on code. *arXiv preprint arXiv:2107.03374*.
   - https://arxiv.org/abs/2107.03374

2. **SWE-bench (real-world bugs):**
   - Jimenez, C. E., Yang, J., Wettig, A., et al. (2024). SWE-bench: Can Language Models Resolve Real-World GitHub Issues? *ICLR 2024*.
   - https://arxiv.org/abs/2310.06770

3. **LiveCodeBench:**
   - Jain, N., Han, K., Gu, A., et al. (2024). LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code. *arXiv preprint arXiv:2403.07974*.
   - https://arxiv.org/abs/2403.07974

### 2025-11-14-context-engineering-for-real-codebases.md
**Tópicos:** Context engineering, retrieval systems, semantic search

**Referências sugeridas:**
1. **RAG (Retrieval-Augmented Generation):**
   - Lewis, P., Perez, E., Piktus, A., et al. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. *NeurIPS 2020*.
   - https://arxiv.org/abs/2005.11401

2. **Semantic code search:**
   - Husain, H., Wu, H. H., Gazit, T., Allamanis, M., & Brockschmidt, M. (2019). CodeSearchNet Challenge: Evaluating the State of Semantic Code Search. *arXiv preprint arXiv:1909.09436*.
   - https://arxiv.org/abs/1909.09436

3. **Code embeddings:**
   - Feng, Z., Guo, D., Tang, D., et al. (2020). CodeBERT: A pre-trained model for programming and natural languages. *EMNLP 2020*.
   - https://arxiv.org/abs/2002.08155

### 2025-11-23-future-of-ai-pair-programming.md
**Tópicos:** AI agents, autonomous systems, pair programming

**Referências sugeridas:**
1. **Multi-agent systems:**
   - Qian, C., Cong, X., Yang, C., et al. (2023). Communicative Agents for Software Development. *arXiv preprint arXiv:2307.07924*.
   - https://arxiv.org/abs/2307.07924

2. **AI pair programming effectiveness:**
   - Vaithilingam, P., Zhang, T., & Glassman, E. L. (2022). Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models. *CHI 2022*.
   - https://doi.org/10.1145/3491101.3519665

3. **Agent-based development:**
   - Hong, S., Zheng, X., Chen, J., et al. (2023). MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. *arXiv preprint arXiv:2308.00352*.
   - https://arxiv.org/abs/2308.00352

### 2025-11-24-complete-workflow-guide.md
**Tópicos:** Software development workflows, TDD, agile practices

**Referências sugeridas:**
1. **Test-Driven Development:**
   - Beck, K. (2003). *Test-Driven Development: By Example*. Addison-Wesley Professional.
   - ISBN: 978-0321146533

2. **Agile workflows:**
   - Fowler, M., & Highsmith, J. (2001). The agile manifesto. *Software Development*, 9(8), 28-35.
   - https://agilemanifesto.org/

3. **Code quality and refactoring:**
   - Fowler, M. (2018). *Refactoring: Improving the Design of Existing Code* (2nd ed.). Addison-Wesley Professional.
   - ISBN: 978-0134757599

### 2025-11-15-groq-optimal-configs.md
**Tópicos:** Model configuration, optimization, API usage

**Referências sugeridas:**
1. **LLM optimization strategies:**
   - Zhao, W. X., Zhou, K., Li, J., et al. (2023). A Survey of Large Language Models. *arXiv preprint arXiv:2303.18223*.
   - https://arxiv.org/abs/2303.18223

2. **Cost-performance tradeoffs:**
   - Sheng, Y., Zheng, L., Yuan, B., et al. (2023). High-Throughput Generative Inference of Large Language Models with a Single GPU. *ICML 2023*.
   - https://arxiv.org/abs/2303.06865

## Critérios de seleção

✅ **Incluir referências quando:**
- Mencionar técnicas/algoritmos específicos (PageRank, RAG, KAN)
- Citar benchmarks conhecidos (HumanEval, SWE-Bench)
- Basear decisões de design em papers (unbiased sampling)
- Fazer claims técnicos que se beneficiam de autoridade

❌ **Não incluir quando:**
- Post é puramente tutorial/how-to
- Conteúdo é opinião/experiência pessoal
- Conceitos são básicos/amplamente conhecidos

## Próximos passos

1. **Revisar e aprovar** esta lista
2. **Adicionar referências** aos posts selecionados
3. **Formato consistente:** usar footnotes [^1] como nos posts de ensemble
4. **Seção References** no final de cada post com citações completas
